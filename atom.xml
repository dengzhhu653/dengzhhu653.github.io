<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title></title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://dengzhhu653.github.io/"/>
  <updated>2016-09-14T06:24:25.000Z</updated>
  <id>https://dengzhhu653.github.io/</id>
  
  <author>
    <name>Zhihua Deng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>RegionServer不断重建Stream Pipeline问题</title>
    <link href="https://dengzhhu653.github.io/blog/2016-08-18-streaming-pipeline/"/>
    <id>https://dengzhhu653.github.io/blog/2016-08-18-streaming-pipeline/</id>
    <published>2016-09-13T11:47:34.000Z</published>
    <updated>2016-09-14T06:24:25.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="&#x73B0;&#x8C61;"><a href="#&#x73B0;&#x8C61;" class="headerlink" title="&#x73B0;&#x8C61;"></a>&#x73B0;&#x8C61;</h3><p>&#x5728;&#x6211;&#x4EEC;&#x7684;Hbase&#x96C6;&#x7FA4;&#x4E2D;&#xFF0C;&#x6709;&#x65F6;&#x5B58;&#x5728;&#x6709;&#x4E9B;<em>RegionServer</em> &#x56E0;&#x4E3A;&#x4E0D;&#x80FD;&#x7EE7;&#x7EED;&#x5F80;<em>HDFS</em> &#x4E2D;&#x5199;&#x5165;<strong>WAL</strong>&#x6570;&#x636E;&#x800C;&#x5BFC;&#x81F4;&#x5F02;&#x5E38;&#x9000;&#x51FA;&#xFF0C;&#x76F8;&#x5E94;&#x7684;&#x5F02;&#x5E38;&#x5982;&#x4E0B;&#xFF1A;</p>
<pre>
2016-08-06 03:45:42,547 FATAL [regionserver/c1-hd-dn18.bdp.idc/10.130.1.37:16020.logRoller] regionserver.HRegionServer: ABORTING region server c1-hd-dn18.bdp.idc,16020,1469772903345: Failed log close in log roller
org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException: hdfs://ns1/hbase/WALs/c1-hd-dn18.bdp.idc,16020,1469772903345/c1-hd-dn18.bdp.idc%2C16020%2C1469772903345.default.1470426151350, unflushedEntries=61
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.replaceWriter(FSHLog.java:988)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:721)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:137)
        at java.lang.Thread.run(Thread.java:745)
</pre>
&#x4EA7;&#x751F;&#x8FD9;&#x4E2A;FATAL&#x4E4B;&#x524D;&#xFF0C;&#x5927;&#x91CF;&#x7684; _&quot;java.io.IOException:Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try.&quot;_ &#x8BB0;&#x5F55;&#x5728;&#x5176;&#x65E5;&#x5FD7;&#x6587;&#x4EF6;&#x4E2D;&#x3002; &#x4ECE;&#x8BE5;&#x5F02;&#x5E38;&#x629B;&#x51FA;&#x7684;message&#x6765;&#x5206;&#x6790;&#xFF0C; &#x5219;&#x610F;&#x5473;&#x7740;&#x5728; _RegionServer_ &#x8F93;&#x51FA;&#x8FD9;&#x6837;&#x5F02;&#x5E38;&#x7684;&#x90A3;&#x523B;&#x8D77;&#xFF0C; &#x5728;_HDFS_&#x96C6;&#x7FA4;&#x8303;&#x56F4;&#x5185;&#x627E;&#x4E0D;&#x5230;&#x4E00;&#x4F8B;&#x53EF;&#x7528;&#x7684;DataNode&#x6765;&#x52A0;&#x5165;&#x5230;&#x5F53;&#x524D;&#x7684;_Stream Pipeline_ &#x4E2D;&#x3002; &#x7136;&#x800C;&#x4E8B;&#x5B9E;&#x4E0A;&#xFF0C;&#x6211;&#x4EEC;&#x5F53;&#x65F6;&#x7684;_HDFS_&#x8FD8;&#x5728;&#x6B63;&#x5E38;&#x63D0;&#x4F9B;&#x7740;&#x6570;&#x636E;&#x7684;&#x589E;&#x5220;&#x6539;&#x67E5;&#x529F;&#x80FD;&#xFF0C;&#x5E76;&#x975E;&#x6CA1;&#x6709;&#x6B63;&#x5E38;&#x7684;_DataNode_&#x53EF;&#x7528;&#x3002;

&#x7EE7;&#x7EED;&#x5206;&#x6790;_RegionServer_&#x7684;&#x65E5;&#x5FD7;&#xFF0C;&#x5728;&#x65E0;&#x53EF;&#x7528;_DataNode_&#x4E4B;&#x524D;&#xFF0C;_RegionServer_&#x4F1A;&#x4E0D;&#x65AD;&#x5C1D;&#x8BD5;&#x4E0E;&#x65B0;&#x7684;_DataNode_&#x91CD;&#x5EFA;Stream Pipeline&#xFF0C;&#x6BEB;&#x65E0;&#x4F8B;&#x5916;&#xFF0C; &#x8FD9;&#x6837;&#x7684;&#x5C1D;&#x8BD5;&#x90FD;&#x5931;&#x8D25;&#x4E86;&#xFF1A;
<pre>
2016-08-06 03:44:29,320 INFO  [DataStreamer for file /hbase/WALs/c1-hd-dn18.bdp.idc,16020,1469772903345/c1-hd-dn18.bdp.idc%2C16020%2C1469772903345.default.1470426151350 block BP-360285305-10.130.1.11-1444619256876:blk_1124743217_51010856] hdfs.DFSClient: Exception in createBlockOutputStream
java.io.IOException: Got error, status message , ack with firstBadLink as 10.130.a.b:50010
        at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:140)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1334)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.(DFSOutputStream.java:1159)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:876)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:402)
2016-08-06 03:44:29,321 WARN  [DataStreamer for file /hbase/WALs/c1-hd-dn18.bdp.idc,16020,1469772903345/c1-hd-dn18.bdp.idc%2C16020%2C1469772903345.default.1470426151350 block BP-360285305-10.130.1.11-1444619256876:blk_1124743217_51010856] hdfs.DFSClient: Error Recovery for block BP-360285305-10.130.1.11-1444619256876:blk_1124743217_51010856 in pipeline DatanodeInfoWithStorage[10.130.x.x:50010,DS-b2197bf5-f129-44df-b569-3ba0e51772c4,DISK], DatanodeInfoWithStorage[10.130.x.x:50010,DS-b0dc4a29-30fe-4633-a292-79274279e345,DISK], DatanodeInfoWithStorage[10.130.a.b:50010,DS-abe5559f-f706-4309-983b-08dd30bcdca4,DISK]: bad datanode DatanodeInfoWithStorage[10.130.a.b:50010,DS-abe5559f-f706-4309-983b-08dd30bcdca4,DISK]
</pre>

<h3 id="&#x5206;&#x6790;"><a href="#&#x5206;&#x6790;" class="headerlink" title="&#x5206;&#x6790;"></a>&#x5206;&#x6790;</h3><p><em>RegionServer(DFSClient)</em> &#x5C06;<em>Bad DataNode</em>&#x52A0;&#x5165;&#x5230;&#x4E00;&#x4E2A;&#x4E0D;&#x53EF;&#x7528;&#x7684;&#x961F;&#x5217;<strong>failed</strong>&#x4E2D;&#xFF0C; &#x5728;&#x5411;<em>NameNode</em> &#x8BF7;&#x6C42;&#x4E00;&#x4E2A;&#x65B0;&#x7684;DataNode&#xFF1A;</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">createBlockOutputStream:</div><div class="line">   <span class="keyword">while</span>(pipeline&#x521B;&#x5EFA;&#x6CA1;&#x6709;&#x6210;&#x529F; &amp;&amp; pipeline&#x6CA1;&#x6709;&#x88AB;&#x5173;&#x95ED; &amp;&amp; dfsclient&#x5728;&#x88AB;&#x4F7F;&#x7528;&#xFF09; &#xFF5B;</div><div class="line">     <span class="comment">//1, &#x5982;&#x679C;&#x6709;DataNode&#x5728;&#x5199;&#x6216;&#x8005;&#x521B;&#x5EFA;pipeline&#x65F6;&#x51FA;&#x73B0;&#x95EE;&#x9898;&#xFF0C;&#x5C06;&#x51FA;&#x9519;&#x7684;DataNode&#x52A0;&#x5165;&#x5230;&#x4E0D;&#x53EF;&#x7528;&#x7684;&#x961F;&#x5217;&#x4E2D;</span></div><div class="line">     failed.add(nodes[errorIndex]);</div><div class="line">     <span class="comment">//2, &#x5C06;&#x6709;&#x95EE;&#x9898;&#x7684;DataNode&#x4ECE;&#x5F53;&#x524D;&#x7684;pipeline&#x4E2D;&#x79FB;&#x9664;</span></div><div class="line">     <span class="comment">//3, &#x662F;&#x5426;&#x9700;&#x8981;&#x5F80;pipeline&#x4E2D;&#x6DFB;&#x52A0;&#x65B0;&#x7684;DataNode&#x8282;&#x70B9;</span></div><div class="line">     <span class="keyword">if</span>(&#x7B26;&#x5408;datanode&#x7684;&#x66FF;&#x6362;&#x7B56;&#x7565;&#xFF09;&#xFF5B;</div><div class="line">        addDatanode2ExistingPipeline</div><div class="line">     &#xFF5D;</div><div class="line">     <span class="comment">//4, &#x751F;&#x6210;&#x65B0;&#x7684;generation stamp&#x7528;&#x4EE5;&#x533A;&#x5206;&#x4E0D;&#x540C;blk&#x7684;&#x4E0D;&#x540C;&#x7248;&#x672C;</span></div><div class="line">     <span class="comment">//5, &#x521B;&#x5EFA;pipeline&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x6D41;</span></div><div class="line">   &#xFF5D;</div><div class="line"> </div><div class="line"> addDatanode2ExistingPipeline:</div><div class="line">    <span class="comment">//get a new datanode</span></div><div class="line">    <span class="keyword">final</span> DatanodeInfo[] original = nodes;</div><div class="line">    <span class="comment">//nodes -&gt; &#x76EE;&#x524D;pipeline&#x4E0A;&#x5305;&#x542B;&#x7684;DataNode&#x8282;&#x70B9;</span></div><div class="line">    <span class="comment">//failed -&gt; &#x7F13;&#x5B58;&#x4E0D;&#x53EF;&#x7528;&#x7684;DataNode&#x7684;&#x5217;&#x8868;</span></div><div class="line">    <span class="comment">//&#x5411;NameNode&#x8BF7;&#x6C42;&#x65B0;&#x7684;DataNode</span></div><div class="line">    <span class="keyword">final</span> LocatedBlock lb = dfsClient.namenode.getAdditionalDatanode(</div><div class="line">        src, fileId, block, nodes, storageIDs,</div><div class="line">        failed.toArray(<span class="keyword">new</span> DatanodeInfo[failed.size()]),</div><div class="line">        <span class="number">1</span>, dfsClient.clientName);</div><div class="line">    setPipeline(lb);</div></pre></td></tr></table></figure>
<p>&#x5728;&#x65B0;pipeline&#x4E2D;&#x6DFB;&#x52A0;&#x7684;DataNode&#x8282;&#x70B9;&#x65E5;&#x5FD7;&#x6587;&#x4EF6;&#x4E2D;&#xFF0C;&#x53D1;&#x73B0;&#x5F53;&#x5199;&#x5165;&#x8FD9;&#x4E2A;&#x5757;&#x65F6;&#xFF0C;&#x7531;&#x4E8E;&#x8BE5;&#x8282;&#x70B9;&#x5E76;&#x6CA1;&#x6709;&#x76F8;&#x5E94;&#x7684;replica&#xFF0C;&#x800C;&#x4E0D;&#x80FD;&#x6267;&#x884C;append&#x7684;&#x64CD;&#x4F5C;&#x3002;&#x8BE5;DataNode&#x88AB;Client&#x6807;&#x8BB0;&#x4E3A;<em>Bad DataNode</em>, &#x4E00;&#x4E2A;&#x65B0;&#x7684;<em>DataNode</em>&#x66FF;&#x6362;&#x8FD9;&#x4E2A;Bad DataNode&#xFF0C;&#x91CD;&#x5EFA;pipeline&#xFF0C;append block&#x64CD;&#x4F5C;&#x5931;&#x8D25;&#xFF0C;&#x91CD;&#x590D;&#x8FD9;&#x6837;&#x7684;&#x64CD;&#x4F5C;, &#x76F4;&#x5230;hdfs&#x96C6;&#x7FA4;&#x8303;&#x56F4;&#x5185;&#x7684; <em>DataNode</em> &#x88AB;&#x8017;&#x5C3D;&#x3002;</p>
<p>DFSClient&#x5728;&#x9009;&#x62E9;&#x65B0;&#x7684;DataNode&#x6062;&#x590D;pipeline&#x4E4B;&#x524D;&#xFF0C;&#x7531;&#x4E8E;&#x8BE5;DataNode&#x4E2D;&#x5E76;&#x6CA1;&#x6709;block&#x76F8;&#x5E94;&#x7684;replica&#xFF0C;&#x9996;&#x5148;&#x4F1A;&#x4ECE;&#x539F;pipeline&#x9009;&#x62E9;&#x4E00;&#x53F0;DataNode&#x4F5C;&#x4E3A;src, &#x5411;src&#x53D1;&#x9001;&#x4E00;&#x4E2A;transfer blk&#x5230;&#x65B0;DataNode&#x7684;&#x4E00;&#x4E2A;RPC&#x8BF7;&#x6C42;&#xFF1A;</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//transfer replica</span></div><div class="line"><span class="keyword">final</span> DatanodeInfo src = d == <span class="number">0</span>? nodes[<span class="number">1</span>]: nodes[d - <span class="number">1</span>];</div><div class="line"><span class="keyword">final</span> DatanodeInfo[] targets = {nodes[d]};</div><div class="line"><span class="keyword">final</span> StorageType[] targetStorageTypes = {storageTypes[d]};</div><div class="line">transfer(src, targets, targetStorageTypes, lb.getBlockToken());</div></pre></td></tr></table></figure>
<p>&#x7531;&#x6B64;&#xFF0C;&#x4FDD;&#x8BC1;pipeline&#x4E0A;&#x6240;&#x6709;&#x7684;datanode&#x90FD;&#x6709;replica&#xFF0C;&#x4FDD;&#x8BC1;append&#x64CD;&#x4F5C;&#x80FD;&#x591F;&#x7EE7;&#x7EED;&#x8FDB;&#x884C;&#x3002;<br>&#x7ED3;&#x5408;&#x65B0;DataNode&#x629B;&#x51FA;&#x7684;&#x5F02;&#x5E38;&#xFF0C;&#x5F88;&#x660E;&#x663E;&#xFF0C; blk&#x5E76;&#x6CA1;&#x6709;&#x88AB;transfer&#x5230;&#x65B0;&#x7684;DataNode&#x8282;&#x70B9;&#x4E0A;&#x3002;</p>
<p>&#x5728;&#x6267;&#x884C;transfer blk&#x64CD;&#x4F5C;&#x7684;src datanode&#x4E0A;&#xFF0C;&#x5BF9;&#x5E94;&#x6709;&#x8FD9;&#x6837;&#x7684;&#x5F02;&#x5E38;&#xFF1A;</p>
<pre>
f8162f70b22;nsid=920937379;c=0):Failed to transfer BP-360285305-10.130.1.11-1444619256876:blk_1124743217_51012555 to 10.130.a.b:50010 got
java.io.IOException: Need 96273147 bytes, but only 96270660 bytes available
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.waitForMinLength(BlockSender.java:475)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:242)
        at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2116)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.transferReplicaForPipelineRecovery(DataNode.java:2866)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.transferBlock(DataXceiver.java:869)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock(Receiver.java:168)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:86)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
        at java.lang.Thread.run(Thread.java:745)
</init></pre>

<p>&#x7ED3;&#x5408;&#x4EE3;&#x7801;&#xFF0C;&#x53EF;&#x77E5;&#xFF1A;<strong>blk_1124743217</strong> &#x51FA;&#x73B0;&#x4E86;[ack bytes] &gt; [bytes on disk]&#x7684;&#x73B0;&#x8C61;&#xFF0C;&#x9020;&#x6210;&#x8FD9;&#x53F0;DataNode&#x65E0;&#x6CD5;&#x5411;10.130.a.b&#x590D;&#x5236;replica&#x7684;&#x95EE;&#x9898;&#x3002;</p>
<p>&#x5F53;DFSClient&#x6BCF;&#x4E00;&#x6B21;&#x521B;&#x5EFA;pipeline&#xFF0C;&#x9009;&#x62E9;&#x8FD9;&#x53F0;&#x672C;&#x8EAB;&#x6709;&#x95EE;&#x9898;&#x7684;DataNode&#x4F5C;&#x4E3A;transfer source&#x65F6;&#xFF0C; &#x90A3;&#x4E48;&#x5728;&#x521D;&#x59CB;&#x5316;pipeline&#x8F93;&#x5165;&#x8F93;&#x51FA;&#x6D41;&#x65F6;(DataStreamer.createBlockOutputStream), &#x7531;&#x4E8E;&#x4E4B;&#x524D;&#x7684;&#x590D;&#x5236;(transfer)&#x64CD;&#x4F5C;&#x5931;&#x8D25;&#xFF0C;pipeline&#x4E0A;&#x6709;&#x4E9B;DataNode&#x5E76;&#x6CA1;&#x6709;replica&#xFF0C;&#x56E0;&#x6B64;writeBlock&#x64CD;&#x4F5C;&#x5E76;&#x4E0D;&#x80FD;&#x5728;pipeline&#x6240;&#x6709;&#x7684;DataNode&#x4E0A;&#x987A;&#x5229;&#x6267;&#x884C;&#xFF0C;pipeline&#x521B;&#x5EFA;&#x5931;&#x8D25;&#x3002;&#x8FD9;&#x6837;&#x7684;&#x73B0;&#x8C61;&#x7C7B;&#x4F3C;&#x4E8E;HDFS-6937&#xFF0C;&#x53EA;&#x4E0D;&#x8FC7;&#x56E0;&#x4E0D;&#x540C;&#xFF0C;&#x7ED3;&#x679C;&#x7C7B;&#x4F3C;&#x3002;</p>
<p>&#x73B0;&#x5728;&#x95EE;&#x9898;&#x53D8;&#x6210;&#xFF0C;&#x4E3A;&#x4EC0;&#x4E48;&#x5728;DN2(&#x6682;&#x4E14;&#x79F0;&#x4E4B;&#x4E3A;)&#x4E2D;&#xFF0C; &#x51FA;&#x73B0;<strong>blk_1124743217</strong> &#x51FA;&#x73B0;&#x4E86;[ack bytes] &gt; [bytes on disk]&#x7684;&#x73B0;&#x8C61;&#xFF1F;<br>&#x8BBE;&#x6700;&#x521D;&#x7684;pipeline&#x4E3A;: client -&gt; DN1 -&gt; DN2 -&gt; DN3.</p>
<p>&#x5728;DN2&#x4E2D;&#xFF0C;&#x7531;&#x4E8E;&#x5176;&#x5E76;&#x975E;&#x4E3A;pipeline&#x4E2D;&#x6700;&#x540E;&#x4E00;&#x4E2A;datanode, <em>RegionServer</em>&#x4E2D;&#x9ED8;&#x8BA4;&#x4F7F;&#x7528;&#x4E86;hflush&#x7684;&#x65B9;&#x5F0F;&#x6765;&#x5199;&#x5165;WAL&#xFF0C; &#x6240;&#x4EE5;&#x5F53;DN2&#x63A5;&#x6536;&#x5230;DN1&#x7684;packet(pkt)&#x65F6;,&#x5C31;&#x5C06;&#x8BE5;pkt&#x52A0;&#x5165;&#x5230;&#x7B49;&#x5F85;ack&#x7684;&#x961F;&#x5217;&#x4E2D;&#x3002;</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">BlockReceiver#receivePacket:</div><div class="line"> // put in queue for pending acks, unless sync was requested</div><div class="line"> if (responder != null &amp;&amp; !syncBlock &amp;&amp; !shouldVerifyChecksum()) {</div><div class="line">   ((PacketResponder) responder.getRunnable()).enqueue(seqno,</div><div class="line">       lastPacketInBlock, offsetInBlock, Status.SUCCESS);</div><div class="line"> }</div></pre></td></tr></table></figure>
<p>&#x63A5;&#x7740;&#x5C06;pkt&#x5199;&#x5165;&#x5230;DN3&#x4E2D;&#xFF0C; </p>
<p>&#x6700;&#x540E;&#xFF0C;&#x5C06;pkt(data+checksum)&#x5199;&#x5165;&#x5230;&#x5BF9;&#x5E94;&#x7684;&#x6587;&#x4EF6;&#x4E2D;&#x3002;&#x5E76;&#x66F4;&#x65B0;replica [bytes on disk]&#x7684;&#x6570;&#x636E;&#x6307;&#x6807;&#xFF1A;</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/// flush entire packet, sync if requested</span></div><div class="line"> flushOrSync(syncBlock);</div><div class="line"> </div><div class="line"> replicaInfo.setLastChecksumAndDataLen(offsetInBlock, lastCrc);</div></pre></td></tr></table></figure>
<p>&#x5F53;DN2 PacketResponder&#x63A5;&#x6536;&#x5230;DN3&#x7684;pkt ack&#x6570;&#x636E;&#x65F6;&#xFF0C;&#x66F4;&#x65B0;replica&#x7684;[ack bytes]&#x7684;&#x6570;&#x636E;&#x6307;&#x6807;&#xFF1A;</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">PipelineAck replyAck = <span class="keyword">new</span> PipelineAck(seqno, replies,</div><div class="line">    totalAckTimeNanos);</div><div class="line"><span class="keyword">if</span> (replyAck.isSuccess()</div><div class="line">    &amp;&amp; offsetInBlock &gt; replicaInfo.getBytesAcked()) {</div><div class="line">  replicaInfo.setBytesAcked(offsetInBlock);</div><div class="line">}</div></pre></td></tr></table></figure>
<p>&#x5176;&#x4E2D;&#xFF0C; DN2&#x5C06;pkt&#x5199;&#x5165;&#x5230;&#x5B58;&#x50A8;&#x4ECB;&#x8D28;&#x4E2D;&#x4E0E;DN2&#x63A5;&#x6536;DN3&#x7684;ack&#x6570;&#x636E;&#xFF0C;&#x8FD9;&#x4E24;&#x4E2A;&#x8FC7;&#x7A0B;&#x662F;&#x5F02;&#x6B65;&#x7684;&#x3002; &#x4E5F;&#x5C31;&#x662F;&#xFF0C;&#x53EF;&#x80FD;&#x5728;&#x67D0;&#x4E00;&#x65F6;&#x523B;&#xFF0C;&#x5728;DN2&#x4E0A;&#xFF0C;&#x51FA;&#x73B0;&#x7C7B;&#x4F3C;&#x4E8E;&#x201C;[ack bytes] &gt; [bytes on disk]&#x201D;&#x7684;&#x73B0;&#x8C61;&#x3002;</p>
<p>&#x5728;DN2&#x65E5;&#x5FD7;&#x4E2D;&#xFF0C;&#x6709;&#x8FD9;&#x6837;&#x7684;&#x4E00;&#x4E2A;&#x5F02;&#x5E38;&#xFF1A;</p>
<pre>
2016-08-06 03:44:26,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-360285305-10.130.1.11-1444619256876:blk_1124743217_51010856
java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
        at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:268)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.adjustCrcChannelPosition(FsDatasetImpl.java:1479)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.adjustCrcFilePosition(BlockReceiver.java:985)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:677)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:849)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:804)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
        at java.lang.Thread.run(Thread.java:745)
</pre>

<p>&#x4ECE;&#x65E5;&#x5FD7;&#x4E2D;&#xFF0C;DN2&#x5728;&#x5199;&#x5165;blk_1124743217 pkt&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x88AB;&#x65E0;&#x60C5;&#x4E2D;&#x65AD;&#xFF0C;&#x8FD9;&#x76F4;&#x63A5;&#x5BFC;&#x81F4;DN2&#x65E0;&#x6CD5;&#x5C06;packet&#x6570;&#x636E;&#x5199;&#x5165;&#x5230;&#x5B58;&#x50A8;&#x4ECB;&#x8D28;&#x4E2D;,&#x9020;&#x6210;&#x4E86;&#x6570;&#x636E;&#x7684;&#x6C38;&#x4E45;&#x4E22;&#x5931;&#x3002;</p>
<h3 id="More"><a href="#More" class="headerlink" title="More"></a>More</h3><p>DN2&#x53EA;&#x6709;&#x5B8C;&#x6210;&#x4EE5;&#x4E0B;&#x8FD9;&#x4E9B;&#x6B65;&#x9AA4;&#x540E;&#xFF0C;&#x624D;&#x51C6;&#x5907;&#x63A5;&#x6536;&#x4E0B;&#x4E00;&#x4E2A;pkt&#xFF1A;</p>
<li>enqueue pkt ack to waiting queue.</li><br><li>flush pkt to downstream datanode.</li><br><li>flush pkt to disk file.</li>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">while (receivePacket() &gt;= 0) { /* Receive until the last packet */ }</div></pre></td></tr></table></figure>
<p>DN2&#x629B;&#x51FA;<strong>java.nio.channels.ClosedByInterruptException</strong>&#x5F02;&#x5E38;&#x65F6;&#xFF0C; Client&#x4E3A;blk_1124743217&#x5EFA;&#x7ACB;&#x7684;pipeline&#x5931;&#x8D25;&#xFF0C;&#x4ECE;&#x65E5;&#x5FD7;&#x4E0A;&#x5206;&#x6790;&#xFF0C;&#x6B64;&#x540E;DN2&#x4E0A;&#x7684;blk_1124743217&#x4FBF;&#x4E0D;&#x5728;&#x6709;&#x6210;&#x529F;&#x7684;&#x6570;&#x636E;&#x5199;&#x5165;&#x64CD;&#x4F5C;&#x3002;&#x56E0;&#x6B64;&#xFF0C; &#x51FA;&#x73B0;&#x201C;[ack bytes] &gt; [bytes on disk]&#x201D;&#x7684;&#x73B0;&#x8C61;&#x65F6;&#x95F4;&#x53EF;&#x786E;&#x5B9A;&#x4E3A;&#x5728;&#x65E5;&#x5FD7;&#x8BB0;&#x5F55;&#x5F02;&#x5E38;&#x7684;&#x7684;&#x65F6;&#x95F4;&#x201D;2016-08-06 03:44:26&#x201C;&#x4E4B;&#x524D;&#x3002;</p>
<p>&#x5BF9;&#x4E8E;&#x4E0A;&#x8FF0;&#x7684;&#x6848;&#x4F8B;&#x6765;&#x8BF4;&#xFF0C;&#x8BBE;pkt&#x662F;&#x539F;pipeline&#x4E2D;&#x6700;&#x540E;&#x88AB;&#x6210;&#x529F;&#x7684;ack&#x7684;packet&#xFF0C;&#x6309;&#x7167;DFSClient&#x7684;&#x903B;&#x8F91;&#xFF0C;&#x8BE5;pipeline&#x81F3;&#x5C11;&#x6709;&#x53E6;&#x4E00;&#x4E2A;packet&#x6B63;&#x5728;&#x5199;&#x6216;&#x5DF2;&#x5728;pipeline&#x4E0A;&#xFF0C;&#x5C06;&#x5176;&#x6807;&#x8BB0;&#x4E3A;pkt0.(<em>pkt&#x5E76;&#x975E;block&#x6700;&#x540E;&#x4E00;&#x4E2A;packet&#xFF0C;&#x800C;&#x4E14;&#x4E0A;&#x6E38;&#x6301;&#x7EED;&#x6709;&#x6570;&#x636E;&#x5199;&#x5165;.</em>)</p>
<blockquote>
<p>&#x5047;&#x8BBE;DN2&#x4E0A;&#x4E2D;&#x65AD;&#x7684;&#x5F02;&#x5E38;&#x5C5E;&#x4E8E;pkt0&#x3002;   </p>
</blockquote>
<p>pkt0&#x88AB;DN3&#x63A5;&#x6536;&#x4E4B;&#x540E;&#xFF0C;&#x5728;DN3&#x7531;&#x4E8E;SocketTimeOutException&#x5F02;&#x5E38;&#x5173;&#x95ED;Socket&#x4E4B;&#x524D;&#x5C06;pkt0 ack&#x6570;&#x636E;&#x53D1;&#x9001;&#x7ED9;DN2, DN2&#x63A5;&#x6536;&#x5230;&#x8FD9;&#x6837;&#x7684;&#x6570;&#x636E;&#x4E4B;&#x540E;&#xFF0C;&#x66F4;&#x65B0;replica ack bytes&#x6307;&#x6807;&#xFF0C; &#x518D;&#x5C06;DN2&#x548C;DN3&#x7684;ack&#x6570;&#x636E;&#x6253;&#x5305;&#x4E00;&#x8D77;&#x53D1;&#x7ED9;DN1. &#x8981;&#x4F7F;pkt&#x6210;&#x4E3A;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x88AB;&#x6210;&#x529F;ack&#x7684;packet&#xFF0C;&#x4E14;DN2&#x51FA;&#x73B0; ack bytes &gt; bytes on disk&#x7684;&#x73B0;&#x8C61;&#xFF0C; &#x5219;&#x53EF;&#x80FD;&#x6709;&#x4EE5;&#x4E0B;&#x51E0;&#x79CD;&#x60C5;&#x51B5;&#xFF1A;<br></p>
<li><br>DN2&#x672A;&#x5C06;pkt0 ack&#x6570;&#x636E;&#x53D1;&#x9001;&#x7ED9;DN1&#x3002; &#x90A3;&#x4E48;&#x5B58;&#x5728;&#x8FD9;&#x6837;&#x7684;&#x5173;&#x7CFB;&#xFF1A; a1 &lt; a2, a1 &#x4E3A;DN1&#x7684;ack bytes&#x6307;&#x6807;&#xFF1B; a2&#x4E3A;DN2&#x7684;ack bytes&#x7684;&#x6307;&#x6807;&#x3002;<br> <strong>&#x5728;&#x540E;&#x6765;&#x7684;DataNode stream recovery&#x64CD;&#x4F5C;&#x4E2D;&#xFF0C; &#x663E;&#x793A;a1=a2, &#x8FD9;&#x6837;&#x7684;&#x60C5;&#x51B5;&#x88AB;&#x6392;&#x9664;&#x3002;</strong><br></li><br><li><br>  DN1&#x672A;&#x5C06;pkt0 ack&#x6570;&#x636E;&#x53D1;&#x9001;&#x7ED9;Client&#x3002;&#x90A3;&#x4E48;DN1&#x5C06;&#x4F1A;&#x88AB;Client&#x6807;&#x8BB0;&#x6210;&#x8981;&#x66FF;&#x6362;&#x7684;DataNode,&#x800C;&#x5E76;&#x975E;DN3&#x4E86;&#xFF0C;&#x800C;&#x4E14;Client&#x91CD;&#x5EFA;pipeline&#x7684;&#x539F;&#x56E0;&#x4E5F;&#x662F;&#x56E0;&#x4E3A;DN3&#x7684;SocketTimeoutException&#xFF0C;&#x8BE5;&#x4FE1;&#x606F;&#x6CBF;ack&#x4F20;&#x9012;&#x8DEF;&#x5F84;&#x7ECF;DN2 -&gt; DN1&#x4F20;&#x9012;&#x7ED9;Client&#xFF0C;Client&#x624D;&#x77E5;&#x9053;DN3&#x51FA;&#x4E86;&#x95EE;&#x9898;&#x3002;<br></li>   

<blockquote>
<p>&#x56E0;&#x6B64;&#xFF0C;&#x53EF;&#x4EE5;&#x4E0B;&#x4E00;&#x4E2A;&#x8FD9;&#x6837;&#x7684;&#x7ED3;&#x8BBA;&#xFF1A;<strong>&#x5728;DN2&#x4E0A;&#x4E2D;&#x65AD;&#x7684;pkt&#x4E00;&#x5B9A;&#x662F;&#x539F;pipeline&#x4E2D;&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x88AB;&#x6210;&#x529F;ack&#x7684;packet&#x3002;</strong></p>
</blockquote>
<p>&#x8FD9;&#x4E2A;&#x7ED3;&#x8BBA;&#x5F88;&#x91CD;&#x8981;&#xFF0C;&#x53EF;&#x4EE5;&#x5927;&#x81F4;&#x5F97;&#x5230;pipeline(DN1-&gt;DN2-&gt;DN3)&#x51FA;&#x73B0;&#x95EE;&#x9898;&#x90A3;&#x4E00;&#x523B;DN2&#x5904;&#x7406;pkt&#x65F6;&#x7EBF;&#x7A0B;&#x6808;&#x7684;&#x60C5;&#x51B5;&#xFF0C;&#x5728;BlockReceiver&#xFF03;receivePacket&#x65B9;&#x6CD5;&#x4E2D;&#xFF0C;&#x5F80;DN3&#x5199;&#x5165;pkt&#x4E4B;&#x540E;&#xFF0C;adjustCrcFilePosition()&#x65B9;&#x6CD5;&#x4E4B;&#x524D;&#xFF0C;&#x662F;&#x6CA1;&#x6709;&#x6BD4;&#x8F83;&#x8017;&#x65F6;&#x64CD;&#x4F5C;&#x7684;&#x3002;&#x56E0;&#x6B64;&#xFF0C;DN2&#x5F53;&#x65F6;&#x6808;&#x7684;&#x60C5;&#x51B5;&#x5E94;&#x8BE5;&#x662F;&#xFF1A;</p>
<pre>
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
        at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:268)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.adjustCrcChannelPosition(FsDatasetImpl.java:1479)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.adjustCrcFilePosition(BlockReceiver.java:985)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:677)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:849)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:804)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
        at java.lang.Thread.run(Thread.java:745)
</pre>

<p>&#x7531;&#x4E8E;DN2&#x4E00;&#x76F4;&#x9677;&#x5165;&#x6587;&#x4EF6;&#x5BFB;&#x5740;&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;(&#x5904;&#x7406;pkt&#xFF09;&#xFF0C;&#x9020;&#x6210;DN2&#x65E0;&#x6CD5;&#x5904;&#x7406;pkt0&#xFF0C;&#x8FDB;&#x800C;&#x65E0;&#x6CD5;&#x5C06;pkt0&#x5199;&#x5165;&#x5230;DN3&#x4E2D;&#xFF0C;&#x5F53;&#x8D85;&#x8FC7;60s&#x65F6;&#xFF08;rpc timeout&#xFF09;&#xFF0C;DN3&#x7387;&#x5148;&#x629B;&#x51FA;SocketTimeoutException&#x5F02;&#x5E38;&#xFF0C;&#x5C06;pipeline&#x7684;socket&#x8D44;&#x6E90;&#x5173;&#x95ED;&#xFF0C;DN2&#x6355;&#x83B7;&#x5230;&#x8FD9;&#x4E00;&#x4E8B;&#x4EF6;&#x3002;</p>
<p>DN3:</p>
<pre>
2016-08-06 03:44:22,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-360285305-10.130.1.11-1444619256876:blk_1124743217_51010856
java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/DN3:50010 remote=/DN2:43529]
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:472)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:849)
</pre>

<p>DN2:</p>
<pre>
2016-08-06 03:44:22,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-360285305-10.130.1.11-1444619256876:blk_1124743217_51010856, type=HAS_DOWNSTREAM_IN_PIPELINE
java.io.EOFException: Premature EOF: no length prefix available
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2280)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:244)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1237)
        at java.lang.Thread.run(Thread.java:745)
</pre>

<p>PBHelper.vintPrefixed:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> InputStream <span class="title">vintPrefixed</span><span class="params">(<span class="keyword">final</span> InputStream input)</span></span></div><div class="line">     <span class="keyword">throws</span> IOException {</div><div class="line">  <span class="keyword">final</span> <span class="keyword">int</span> firstByte = input.read();</div><div class="line">  </div><div class="line">  <span class="keyword">if</span> (firstByte == -<span class="number">1</span>) {</div><div class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> EOFException(<span class="string">&quot;Premature EOF: no length prefix available&quot;</span>);</div><div class="line">  }</div></pre></td></tr></table></figure>
<p>DN2&#x7684;PacketResponder&#x6536;&#x5230;DN3&#x7684;&#x5F02;&#x5E38;&#x6D88;&#x606F;&#x4E4B;&#x540E;&#xFF0C;&#x5C06;DN3&#x8FD9;&#x53F0;DataNode&#x6807;&#x8BB0;&#x4E3A;Error&#xFF0C;&#x5E76;reply&#x7ED9;DN1, DN1&#x5C06;&#x9519;&#x8BEF;&#x4FE1;&#x606F;&#x5C01;&#x88C5;&#x540E;&#x4EA4;&#x7ED9;Client:</p>
<pre><code><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (ack == <span class="keyword">null</span>) {</div><div class="line">   <span class="comment">// A new OOB response is being sent from this node. Regardless of</span></div><div class="line">   <span class="comment">// downstream nodes, reply should contain one reply.</span></div><div class="line">   replies = <span class="keyword">new</span> <span class="keyword">int</span>[] { myHeader };</div><div class="line"> <span class="comment">//&#x8FD9;&#x91CC;mirrorError &#xFF1D; true;</span></div><div class="line"> } <span class="keyword">else</span> <span class="keyword">if</span> (mirrorError) { <span class="comment">// ack read error</span></div><div class="line">   <span class="keyword">int</span> h = PipelineAck.combineHeader(datanode.getECN(), Status.SUCCESS);</div><div class="line">   <span class="keyword">int</span> h1 = PipelineAck.combineHeader(datanode.getECN(), Status.ERROR);</div><div class="line">   replies = <span class="keyword">new</span> <span class="keyword">int</span>[] {h, h1};</div><div class="line"> }</div></pre></td></tr></table></figure>
</code></pre><p>Client&#x89E3;&#x6790;DN1&#x4F20;&#x6765;&#x7684;ack&#x4FE1;&#x606F;(seqno=-2), &#x53D1;&#x73B0;DN3&#x5BF9;&#x5E94;&#x7684;ack&#x72B6;&#x6001;&#x4E3A;Error&#x7684;&#xFF0C;Client&#x5C06;DN3&#x6807;&#x8BB0;&#x4E3A;&#x4E00;&#x4E2A;&#x4E0D;&#x53EF;&#x7528;&#x7684;<em>DataNode</em>, &#x5E76;&#x5C06;&#x5BF9;&#x5E94;&#x7684;ResponseProcessor&#x7EBF;&#x7A0B;&#x5173;&#x95ED;&#xFF0C;</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// if the Responder encountered an error, shutdown Responder</span></div><div class="line">    <span class="keyword">if</span> (hasError &amp;&amp; response != <span class="keyword">null</span>) {</div><div class="line">      <span class="keyword">try</span> {</div><div class="line">        response.close();</div><div class="line">        response.join();</div><div class="line">        response = <span class="keyword">null</span>;</div><div class="line">      } <span class="keyword">catch</span> (InterruptedException  e) {</div><div class="line">        DFSClient.LOG.warn(<span class="string">&quot;Caught exception &quot;</span>, e);</div><div class="line">    }</div></pre></td></tr></table></figure>
<p>&#x5728;DataStreamer&#x4E3B;&#x7EBF;&#x7A0B;&#x4E0A;&#xFF0C;&#x5C06;&#x539F;&#x6765;&#x7684;pipeline&#x5173;&#x95ED;&#xFF0C;&#x5C06;&#x7B49;&#x5F85;ack&#x7684;packet&#x79FB;&#x5230;&#x8981;&#x53D1;&#x9001;&#x7684;&#x961F;&#x5217;&#x961F;&#x9996;&#x4E2D;,&#x91CD;&#x65B0;&#x9009;&#x62E9;DataNode(&#x66FF;&#x6362;DN3)&#x5EFA;&#x7ACB;pipeline. &#x5728;&#x6062;&#x590D;pipeline&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x4F7F;&#x7528;DN2&#x4F5C;&#x4E3A;transfer source&#xFF0C; &#x800C;&#x539F;&#x6765;DN2&#x4E2D;&#x7684;replica&#x662F;&#x6709;&#x95EE;&#x9898;&#x7684;(&#x6570;&#x636E;&#x4E22;&#x5931;), &#x8FD9;&#x6837;&#x4FBF;&#x9020;&#x6210;&#x65B0;&#x7684;pipeline&#x65E0;&#x6CD5;&#x521B;&#x5EFA;&#x6210;&#x529F;&#x3002;</p>
<h3 id="&#x603B;&#x7ED3;"><a href="#&#x603B;&#x7ED3;" class="headerlink" title="&#x603B;&#x7ED3;"></a>&#x603B;&#x7ED3;</h3><p>1&#xFF0C; Client flushes pkt to pipeline and gets succuss acks from DN1. <br><br>    (Stage -&gt; BlockConstructionStage.DATA_STREAMING)</p>
<p><img src="/blog/2016-08-18-streaming-pipeline/../../images/snap.png" alt=""></p>
<p>2, Client flushes next pkt(pkt0) to pipeline and timeout from DN3. DN2 sends the error ack to DN1.<br><img src="/blog/2016-08-18-streaming-pipeline/../../images/pkt0.png" alt=""></p>
<p>3, Client close current pipeline and choose DN4 to replace DN3, and transfer blk from DN2 to DN4.<br><br>   (Stage-&gt; BlockConstructionStage.PIPELINE_SETUP_STREAMING_RECOVERY)<br><img src="/blog/2016-08-18-streaming-pipeline/../../images/transfer.png" alt=""></p>
<p>4, Create new pipeline with DN1, DN2 and DN4.<br><br>   (Stage-&gt; BlockConstructionStage.DATA_STREAMING)<br><img src="/blog/2016-08-18-streaming-pipeline/../../images/pipeline_new.png" alt=""></p>
<p>&#x540C;&#x65F6;&#xFF0C;&#x901A;&#x8FC7;&#x5206;&#x6790;&#xFF0C;&#x5728;&#x5F80;pipeline&#x6DFB;&#x52A0;DataNode&#x65F6;&#xFF0C;&#x5C3D;&#x7BA1;DataNode&#x5728;tranfer block&#x8FC7;&#x7A0B;&#x4E2D;&#x53D1;&#x751F;IOException(&#x4F8B;&#x5982;&#x672C;&#x4F8B;&#x7684;&#x4F8B;&#x5B50;), &#x7136;&#x800C;Client&#x662F;&#x611F;&#x77E5;&#x4E0D;&#x5230;&#x7684;&#xFF0C;&#x4F7F;&#x7528;&#x6709;&#x95EE;&#x9898;&#x7684;DataNode(&#x6570;&#x636E;&#x5DF2;&#x53D1;&#x751F;&#x4E22;&#x5931;)&#x4EE5;&#x53CA;&#x65B0;DataNode(BlockPool&#x4E2D;&#x6CA1;&#x6709;&#x76F8;&#x5E94;&#x7684;replica)&#x91CD;&#x5EFA;pipeline&#xFF0C;&#x5F80;pipeline&#x5199;&#x5165;&#x6570;&#x636E;&#x65F6;(&#x51C6;&#x786E;&#x6765;&#x8BF4;&#xFF0C;&#x662F;&#x53D1;&#x9001;&#x4E00;&#x4E2A;&#x5199;block&#x7533;&#x8BF7;&#xFF0C;&#x83B7;&#x5F97;&#x8BFB;&#x5199;&#x6570;&#x636E;&#x6D41;)&#xFF0C;&#x662F;&#x4E0D;&#x4F1A;&#x6210;&#x529F;&#x7684;&#x3002;&#x5728;&#x7B26;&#x5408;DataNode&#x66FF;&#x6362;&#x7B56;&#x7565;&#x7684;&#x524D;&#x63D0;&#x4E0B;&#xFF0C;Client&#x4F1A;&#x5C1D;&#x8BD5;&#x9009;&#x62E9;&#x4E0E;HDFS&#x96C6;&#x7FA4;&#x8303;&#x56F4;&#x5185;&#x6240;&#x6709;&#x53EF;&#x7528;&#x7684;DataNode&#x5EFA;&#x7ACB;&#x4E00;&#x6761;pipeline&#xFF0C;&#x76F4;&#x5230;&#x65E0;DataNode&#x53EF;&#x7528;&#x4E3A;&#x6B62;&#x3002;<br>&#x6587;&#x4E2D;&#x63CF;&#x8FF0;&#x7684;&#x5F02;&#x5E38;&#x573A;&#x666F;&#x6309;&#x7167;pipeline&#x8BFB;&#x5199;&#x903B;&#x8F91;&#x662F;&#x5F88;&#x96BE;&#x53D1;&#x751F;&#x7684;&#xFF0C;&#x7136;&#x800C;&#x4E0D;&#x540C;&#x7684;&#x8F6F;&#x786C;&#x4EF6;&#x73AF;&#x5883;&#xFF0C;&#x4E0D;&#x540C;&#x7684;&#x8BFB;&#x5199;&#x538B;&#x529B;&#x7B49;&#x7B49;&#xFF0C;&#x8FD9;&#x4E9B;&#x90FD;&#x4E3A;&#x8F6F;&#x4EF6;&#x7684;&#x6D4B;&#x8BD5;&#x548C;&#x7F16;&#x5199;&#x5E26;&#x6765;&#x4E00;&#x5B9A;&#x7684;&#x5C40;&#x9650;&#x6027;&#xFF0C;&#x4E5F;&#x7ED9;&#x7A0B;&#x5E8F;&#x7684;&#x5065;&#x58EE;&#x6027;&#x5E26;&#x6765;&#x4E00;&#x4E9B;&#x6311;&#x6218;&#x3002;&#x4EE5;&#x6587;&#x4E2D;&#x4E3A;&#x4F8B;&#xFF0C;&#x5F53;&#x53D1;&#x73B0;&#x65E0;&#x53EF;&#x7528;&#x7684;DataNode&#x65F6;&#xFF0C;RegionServer&#x5C06;&#x4F1A;&#x5F02;&#x5E38;&#x9000;&#x51FA;&#x3002;&#x5728;&#x5206;&#x5E03;&#x5F0F;&#x73AF;&#x5883;&#x4E0B;&#xFF0C;&#x8C03;&#x8BD5;&#x548C;&#x5B9A;&#x4F4D;&#x95EE;&#x9898;&#x53D8;&#x5F97;&#x590D;&#x6742;&#x548C;&#x4E0D;&#x786E;&#x5B9A;&#xFF0C;&#x968F;&#x7740;&#x6DF1;&#x5165;&#xFF0C;&#x5047;&#x8BBE;&#x4E0D;&#x65AD;&#x7684;&#x88AB;&#x63A8;&#x7FFB;&#x548C;&#x4FEE;&#x6B63;&#xFF0C;&#x8FD9;&#x4E9B;&#x90FD;&#x6709;&#x8D56;&#x4E8E;&#x65E5;&#x5FD7;&#x548C;&#x6E90;&#x7801;&#x7684;&#x5206;&#x6790;&#x3002;</p>
<h3 id="&#x5982;&#x4F55;&#x89E3;&#x51B3;"><a href="#&#x5982;&#x4F55;&#x89E3;&#x51B3;" class="headerlink" title="&#x5982;&#x4F55;&#x89E3;&#x51B3;"></a>&#x5982;&#x4F55;&#x89E3;&#x51B3;</h3><blockquote>
<p>1, &#x63D0;&#x9AD8;hdfs rpc&#x8BFB;&#x5199;&#x7684;&#x8D85;&#x65F6;&#x65F6;&#x95F4;.<br></p>
</blockquote>
<p>&#x5BF9;&#x4E8E;&#x7B2C;&#x4E00;&#x79CD;&#x89E3;&#x51B3;&#x65B9;&#x6848;&#x6765;&#x8BF4;&#xFF0C;&#x66F4;&#x6539;&#x6D89;&#x53CA;&#x5230;Client&#x7AEF;(<em>RegionServer</em>), &#x540C;&#x65F6;&#x8FD8;&#x9700;&#x8981;&#x91CD;&#x542F;&#x96C6;&#x7FA4;&#x5185;&#x7684;<em>DataNode</em>&#x670D;&#x52A1;&#x3002;&#x8FD9;&#x79CD;&#x65B9;&#x6848;&#x4FEE;&#x6539;&#x7B80;&#x5355;&#xFF0C;&#x7136;&#x800C;&#x5BF9;&#x4E8E;&#x96C6;&#x7FA4;&#x5BB9;&#x9519;&#x548C;&#x6392;&#x9519;&#x6765;&#x8BF4;&#x5E76;&#x975E;&#x6700;&#x4F73;&#xFF0C;&#x540C;&#x65F6;&#x5C06;&#x8FD9;&#x4E2A;&#x503C;&#x8BBE;&#x7F6E;&#x4E3A;&#x591A;&#x5927;&#x6BD4;&#x8F83;&#x5408;&#x9002;&#xFF0C;&#x5219;&#x9700;&#x8981;&#x66F4;&#x5168;&#x9762;&#x7684;&#x6D4B;&#x8BD5;&#x548C;&#x8BC4;&#x4F30;&#x3002;</p>
<blockquote>
<p>2, &#x4FEE;&#x6539;&#x5BA2;&#x6237;&#x7AEF;&#xFF0C;&#x5728;&#x6062;&#x590D;pipeline&#xFF0C;&#x5C06;replica&#x79FB;&#x52A8;&#x5230;&#x65B0;&#x7684;datanode&#x4E4B;&#x95F4;&#xFF0C;&#x4F7F;&#x7528;&#x539F;pipeline&#x6700;&#x540E;&#x4E00;&#x53F0;<em>DataNode</em>&#x4F5C;&#x4E3A;source&#x3002;</p>
</blockquote>
<p>&#x6839;&#x636E;&#x8BBE;&#x8BA1;:</p>
<p>   <img src="/blog/2016-08-18-streaming-pipeline/../../images/client.png" alt=""> </p>
<p>&#x5728;&#x8FD9;&#x91CC;, BA(i, t)&#x8868;&#x793A;&#x5728;&#x4EFB;&#x610F;&#x65F6;&#x523B;t&#xFF0C;pipeline&#x7B2C;i&#x53F0; <em>DataNode</em> acked bytes&#x7684;&#x6307;&#x6807;,  i&#x7684;&#x5927;&#x5C0F;&#x4EE3;&#x8868;&#x79BB;Client&#x7684;&#x8FDC;&#x8FD1;&#xFF1B;BR(i,t)&#x8868;&#x793A;&#x5728;&#x4EFB;&#x610F;&#x65F6;&#x523B;t&#xFF0C;pipeline&#x7B2C;i&#x53F0; <em>DataNode</em> received bytes&#x7684;&#x6307;&#x6807;, i&#x7684;&#x5927;&#x5C0F;&#x4EE3;&#x8868;&#x79BB;Client&#x7684;&#x8FDC;&#x8FD1;. &#x5047;&#x8BBE;&#x539F;pipeline&#x4E2D;&#x6700;&#x540E;&#x4E00;&#x53F0;<em>DataNode</em>, acked bytes&#x6307;&#x6807;&#x8BB0;&#x4E3A;BAD&#xFF0C; received bytes&#x6307;&#x6807;&#x8BB0;&#x4E3A;BR; &#x5BF9;&#x5E94;&#x7684;Client, acked bytes&#x8BB0;&#x4E3A; BAC, &#x53D1;&#x9001;&#x7684;&#x5B57;&#x8282;&#x6570;&#x8BB0;&#x4E3A;BS&#x3002;&#x5219;,&#x5B58;&#x5728;&#x8FD9;&#x6837;&#x7684;&#x4E0D;&#x7B49;&#x5F0F;&#x5173;&#x7CFB;&#xFF1A;<br><strong>BAC &lt;= BAD &lt;= BR &lt;= BS</strong>&#x3002;<br><br>&#x5728;pipeline&#x6700;&#x540E;&#x4E00;&#x53F0;<em>DataNode</em>&#x4E2D;&#xFF0C;&#x5F53;&#x4E14;&#x4EC5;&#x5F53;DataNode&#x5B8C;&#x6210;packet&#x6570;&#x636E;checksum&#x6821;&#x9A8C;&#x4EE5;&#x53CA;&#x5C06;packet flush&#x5230;&#x78C1;&#x76D8;&#x65F6;&#xFF0C;&#x624D;&#x66F4;&#x6539;BAD&#x7684;&#x6570;&#x636E;&#x6307;&#x6807;&#x3002; &#x7531;&#x6B64;&#xFF0C;replica BAD&#x7684;&#x6570;&#x636E;&#x4E00;&#x5B9A;&#x5177;&#x6709;&#x4EE5;&#x4E0B;&#x4E00;&#x4E9B;&#x6027;&#x8D28;&#xFF1A; 1&#xFF0C; replica data&#x4E0E;checksum&#x5339;&#x914D;&#xFF1B; 2&#xFF0C;replica data acked bytes &lt;= data on disk bytes.</p>
<p>&#x5728;&#x6062;&#x590D;pipeline&#x65F6;&#xFF0C; Client&#x5C06;&#x7B49;&#x5F85;ack&#x7684;&#x6570;&#x636E;&#x91CD;&#x65B0;&#x5F52;&#x7F6E;&#x5230;&#x5F85;&#x53D1;&#x9001;&#x5230;&#x961F;&#x5217;&#x4E2D;&#xFF0C;&#x5373;(BAC, BS], &#x4ECE;&#x539F;pipeline&#x6700;&#x540E;<em>DataNode</em>&#x4E0A;&#x62F7;&#x8D1D;&#x7684;&#x6570;&#x636E;&#x533A;&#x95F4;&#x5728;(0, BAD]&#x6216;&#x8005;<br>(0, BR], &#x65B0;&#x9009;&#x62E9;&#x7684;<em>DataNode</em> replica&#x65E0;&#x8BBA;&#x662F;&#x5728;(0, BAD]&#x6216;(0, BR]&#xFF0C; &#x90FD;&#x80FD;&#x6B63;&#x786E;&#x7684;&#x63A5;&#x6536;&#x6765;&#x81EA;Client&#x4ECE;BAC&#x4F4D;&#x7F6E;&#x5F00;&#x59CB;&#x53D1;&#x9001;&#x8FC7;&#x6765;&#x7684;&#x6570;&#x636E;&#x3002;</p>
<p>&#x5047;&#x8BBE;&#x539F;pipeline&#x6700;&#x540E;&#x7684;&#x4E00;&#x53F0;<em>DataNode</em>&#x4E0D;&#x53EF;&#x7528;&#x600E;&#x4E48;&#x529E;? &#x5982;shutdown, abort, replica&#x88AB;&#x5220;&#x9664;&#x7B49;&#x7B49;&#xFF0C;&#x90A3;&#x4E48;&#x91C7;&#x53D6;&#x7684;&#x7B56;&#x7565;&#x662F;&#x968F;&#x673A;&#x4ECE;pipeline&#x5269;&#x4F59;&#x7684;<em>DataNode</em>&#x4F5C;&#x4E3A;&#x65B0;&#x52A0;&#x5165;<em>DataNode</em>&#x7684;transfer source.</p>
<p>&#x540C;&#x6837;&#xFF0C;&#x8BE5;&#x65B9;&#x6848;&#x4E5F;&#x6709;&#x660E;&#x663E;&#x7684;&#x7F3A;&#x70B9;&#xFF0C;&#x5982;&#x679C;&#x53D1;&#x751F;&#x5728;&#x4E0D;&#x540C;&#x673A;&#x67B6;&#x4E0A;&#x7684;&#x6570;&#x636E;&#x62F7;&#x8D1D;&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x6837;&#x7684;&#x65B9;&#x6848;&#x5C06;&#x4F1A;&#x5BF9;pipeline&#x7684;&#x5FEB;&#x901F;&#x6062;&#x590D;&#x6709;&#x4E00;&#x5B9A;&#x7684;&#x6D88;&#x6781;&#x5F71;&#x54CD;&#x3002; &#x4F46;&#x5982;&#x679C;<em>DataNode</em>&#x5728;&#x540C;&#x4E00;rack&#x5185;&#xFF0C;&#x5219;&#x5B8C;&#x5168;&#x6709;&#x7406;&#x7531;&#x8FD9;&#x4E48;&#x505A;&#x3002;&#x4ECE;&#x7406;&#x8BBA;&#x4E0A;&#x6765;&#x8BF4;&#xFF0C;block&#x7684;&#x591A;&#x5907;&#x4EFD;&#x673A;&#x5236;&#x662F;&#x4E3A;&#x4E86;&#x51CF;&#x5C11;&#x5404;&#x79CD;&#x6545;&#x969C;&#x5BFC;&#x81F4;&#x6570;&#x636E;&#x5F02;&#x5E38;&#x800C;&#x91C7;&#x53D6;&#x7684;&#x4E00;&#x79CD;&#x65B9;&#x6848;&#xFF0C;&#x7136;&#x800C;&#xFF0C;&#x8FD9;&#x5E76;&#x4E0D;&#x610F;&#x5473;&#x7740;&#x6570;&#x636E;&#x4F1A;&#x4E00;&#x76F4;&#x6B63;&#x5E38;&#x4E0B;&#x53BB;&#xFF0C;&#x5728;&#x67D0;&#x4E9B;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x91C7;&#x7528;&#x65B9;&#x6848;2&#x4E5F;&#x672A;&#x80FD;&#x9632;&#x6B62;&#x6570;&#x636E;&#x7684;&#x5B8C;&#x6574;&#x6B63;&#x786E;&#x6027;&#xFF0C; &#x4F8B;&#x5982;: &#x6309;&#x7167;&#x63CF;&#x8FF0;&#xFF0C; &#x5F53;DN1&#x548C;DN2&#x540C;&#x65F6;&#x51FA;&#x73B0;data acked bytes &gt; data on disk bytes&#x73B0;&#x8C61;&#xFF0C;&#x800C;&#x6B64;&#x65F6;DN3&#x4E0D;&#x53EF;&#x7528;&#x65F6;&#xFF0C;&#x4E5F;&#x4F1A;&#x51FA;&#x73B0;&#x7C7B;&#x4F3C;&#x4E8E;&#x6587;&#x4E2D;&#x63CF;&#x8FF0;&#x7684;&#x95EE;&#x9898;&#x3002;</p>
<p>&#x6709;&#x65F6;&#x5019;&#x4F1A;&#x6000;&#x7591;&#xFF0C;DataNode&#x662F;&#x5982;&#x4F55;&#x6210;&#x529F;&#x5EFA;&#x7ACB;&#x8D77;pipeline&#x7684;&#xFF1F; &#x5728;DataXceiver&#x8FD9;&#x4E2A;&#x7C7B;&#x4E2D;&#x6C38;&#x8FDC;&#x9009;&#x62E9;&#x7684;&#x662F;targets[0]&#x7684;DataNode&#x4EE3;&#x8868;&#x81EA;&#x5DF1;&#xFF0C;&#x5982;&#x679C;DN2&#x4E5F;&#x9009;&#x62E9;targets[0]&#x4EE3;&#x8868;&#x81EA;&#x5DF1;&#xFF0C;&#x90A3;&#x4E2A;&#x8FD9;&#x4E2A;&#x600E;&#x4E48;&#x4F1A;&#x6210;&#x7ACB;&#x5462;&#xFF1F;&#x8FD9;&#x4E9B;&#x7EC6;&#x8282;&#x4F53;&#x73B0;&#x5728;Sender&#x8FD9;&#x4E2A;&#x7C7B;&#x4E2D;&#xFF0C;&#x5F53;&#x8C03;&#x7528;new Sender(out).writeBlock()&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x8BE5;&#x65B9;&#x6CD5;&#x4ECE;&#x7B2C;&#x4E00;&#x4E2A;&#x4F4D;&#x7F6E;&#x5F00;&#x59CB;&#x5C06;targets&#x4E2D;&#x7684;&#x6570;&#x636E;&#x62F7;&#x8D1D;&#x5230;&#x53E6;&#x4E00;&#x4E2A;&#x6570;&#x7EC4;&#x4E2D;&#xFF0C;&#x5982;&#x679C;&#x8BF4;targets[0]&#x4EE3;&#x8868;&#x81EA;&#x5DF1;&#xFF0C;&#x90A3;&#x4E48;targets[1]&#x5219;&#x4EE3;&#x8868;&#x7684;&#x662F;&#x4E0B;&#x6E38;&#x8282;&#x70B9;&#xFF0C;&#x5C06;&#x4E00;&#x4E9B;&#x6570;&#x636E;&#x62F7;&#x8D1D;&#x5B8C;&#x6210;&#x540E;&#xFF0C;&#x5728;&#x901A;&#x8FC7;&#x534F;&#x8BAE;&#xFF0C;&#x5199;&#x5165;&#x5230;&#x8F93;&#x51FA;&#x6D41;&#x4E2D;&#x3002;</p>
<p>new Sender(out).writeBlock()</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">OpWriteBlockProto.Builder proto = OpWriteBlockProto.newBuilder()</div><div class="line">      .setHeader(header)</div><div class="line">      .setStorageType(PBHelper.convertStorageType(storageType))</div><div class="line">      <span class="comment">//&#x4ECE;targets&#x7B2C;&#x4E00;&#x4E2A;&#x4F4D;&#x7F6E;&#x5F00;&#x59CB;&#xFF0C;&#x62F7;&#x8D1D;DatanodeInfo&#x4FE1;&#x606F;&#x5230;&#x53E6;&#x4E00;&#x4E2A;list&#x4E2D;</span></div><div class="line">      .addAllTargets(PBHelper.convert(targets, <span class="number">1</span>))</div><div class="line">      .addAllTargetStorageTypes(PBHelper.convertStorageTypes(targetStorageTypes, <span class="number">1</span>))</div></pre></td></tr></table></figure>
<p>PBHelper.convert(targets, 1&#xFF09;</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;? extends HdfsProtos.DatanodeInfoProto&gt; convert(</div><div class="line">      DatanodeInfo[] dnInfos, <span class="keyword">int</span> startIdx) {</div><div class="line">    <span class="keyword">if</span> (dnInfos == <span class="keyword">null</span>)</div><div class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">    ArrayList&lt;HdfsProtos.DatanodeInfoProto&gt; protos = Lists</div><div class="line">        .newArrayListWithCapacity(dnInfos.length);</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = startIdx; i &lt; dnInfos.length; i++) {</div><div class="line">      protos.add(convert(dnInfos[i]));</div><div class="line">    }</div><div class="line">    <span class="keyword">return</span> protos;</div><div class="line">  }</div></pre></td></tr></table></figure>
<p>&#x5047;&#x8BBE;DN2&#x51FA;&#x73B0;&#x4E00;&#x4E2A;&#x95EE;&#x9898;&#xFF0C;&#x6309;&#x7167;&#x8FD9;&#x6837;&#x7684;&#x65B9;&#x6848;&#xFF0C;DN3&#x4F1A;&#x5C06;&#x5C06;block&#x590D;&#x5236;&#x5230;DN4&#x4E2D;&#xFF0C;&#x91CD;&#x65B0;&#x5EFA;&#x7ACB;&#x8D77;&#x4E00;&#x6761;&#x65B0;&#x7684;Pipeline(DN1 -&gt; DN2 -&gt; DN4)&#x3002;&#x6309;&#x7167;&#x6587;&#x4E2D;&#x7684;&#x63CF;&#x8FF0;&#xFF0C; &#x5728;DN2&#x4E0A;&#x51FA;&#x73B0;&#x4E86;replica.getVisibleLength() &gt; replica.getBytesOnDisk()&#x7684;&#x73B0;&#x8C61;&#xFF0C; &#x5F53;client&#x5F80;&#x65B0;&#x5EFA;&#x7684;pipeline&#x4E2D;&#x5199;&#x5165;&#x6570;&#x636E;&#x7684;&#x65F6;&#x5019;&#xFF0C;DN2&#x4F1A;&#x5982;&#x4F55;&#x5904;&#x7406;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#xFF1F;</p>
<p>DataReceiver#receivePacket():</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// The data buffer position where write will begin. If the packet</span></div><div class="line"><span class="comment">// data and on-disk data have no overlap, this will not be at the</span></div><div class="line"><span class="comment">// beginning of the buffer.</span></div><div class="line"><span class="keyword">int</span> startByteToDisk = (<span class="keyword">int</span>)(onDiskLen-firstByteInBlock) </div><div class="line">    + dataBuf.arrayOffset() + dataBuf.position();</div><div class="line"></div><div class="line"><span class="comment">// Actual number of data bytes to write.</span></div><div class="line"><span class="keyword">int</span> numBytesToDisk = (<span class="keyword">int</span>)(offsetInBlock-onDiskLen);</div><div class="line"></div><div class="line"><span class="comment">// Write data to disk.</span></div><div class="line"><span class="keyword">long</span> begin = Time.monotonicNow();</div><div class="line">out.write(dataBuf.array(), startByteToDisk, numBytesToDisk);</div></pre></td></tr></table></figure>
<p>firstByteInBlock&#x662F;&#x8BE5;packet&#x4E2D;&#x7B2C;&#x4E00;&#x4E2A;&#x5B57;&#x8282;&#x7684;offset&#xFF0C; offsetInBlock&#x662F;firstByteInBlock + packet length; &#x6839;&#x636E;&#x8BA1;&#x7B97;&#x7ED3;&#x679C;&#xFF0C;&#x5F53;onDiskLen &lt; firstByteInBlock&#x65F6;&#xFF0C;&#x6B64;&#x65F6;startByteToDisk&#x662F;&#x4E00;&#x4E2A;&#x8D1F;&#x6570;&#xFF0C;&#x8F93;&#x51FA;&#x6D41;&#x5199;&#x6570;&#x636E;&#x65F6;&#xFF0C;&#x4F1A;&#x51FA;&#x73B0;&#x5982;&#x4E0B;&#x5F02;&#x5E38;&#xFF1A;</p>
<pre>
2016-08-24 16:33:09,415 ERROR datanode.DataNode (DataXceiver.java:run(278)) - 127.0.0.1:51033:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51047 dst: /127.0.0.1:51033
java.lang.IndexOutOfBoundsException
    at java.io.FileOutputStream.writeBytes(Native Method)
    at java.io.FileOutputStream.write(FileOutputStream.java:326)
    at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:648)
    at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:849)
    at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:804)
    at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
    at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
    at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)
    at java.lang.Thread.run(Thread.java:745)
</pre>

<p>Client&#x6536;&#x5230;DN2&#x7684;ack error, &#x91CD;&#x65B0;&#x5EFA;&#x7ACB;&#x4E00;&#x6761;&#x65B0;&#x7684;pipeline&#xFF0C; &#x8FD9;&#x4E2A;&#x8FC7;&#x7A0B;&#x5E76;&#x4E0D;&#x4F1A;&#x4EA7;&#x751F;&#x6570;&#x636E;&#x4E22;&#x5931;&#x7684;&#x60C5;&#x51B5;&#x3002; &#x5F53;&#x7136;&#xFF0C;&#x5728;&#x5B9E;&#x9645;&#x7684;&#x751F;&#x4EA7;&#x73AF;&#x5883;&#x4E2D;&#xFF0C;&#x6211;&#x4EEC;&#x8FD8;&#x53D1;&#x73B0;&#xFF0C;&#x5F53;DN2&#x51FA;&#x73B0;IO&#x74F6;&#x9888;&#x95EE;&#x9898;&#x65F6;&#xFF0C;&#x91CD;&#x65B0;&#x5EFA;&#x7ACB;pipeline&#xFF0C;&#x5199;&#x5165;block&#x65F6;&#xFF0C;&#x5E76;&#x6CA1;&#x6709;&#x5BF9;&#x5E94;&#x7684;java.lang.IndexOutOfBoundsException&#x5F02;&#x5E38;&#x629B;&#x51FA;&#x6765;&#x3002;&#x800C;&#x662F;&#x7EBF;&#x7A0B;&#x88AB;IO&#x963B;&#x585E;&#x4F4F;&#xFF0C;&#x8FD8;&#x6CA1;&#x6709;&#x8D70;&#x5230;&#x4E0A;&#x9762;&#x63CF;&#x8FF0;&#x7684;&#x8FC7;&#x7A0B;&#xFF1A;</p>
<p><pre><br>2016-09-07 09:56:45,521 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Join on writer thread Thread[DataXceiver for client DFSClient<em>NONMAPREDUCE</em>-1754701259_1 at /10.130.1.37:51509 [Receiving block BP-360285305-10.130.1.11-1444619256876:blk_1143909363_70177457],5,dataXceiverServer] timed out<br>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getTmpInputStreams(FsDatasetImpl.java:751)<br>org.apache.hadoop.hdfs.server.datanode.BlockReceiver.computePartialChunkCrc(BlockReceiver.java:1023)<br>org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:634)<br>org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:849)<br>org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:804)<br>org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)<br>org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)<br>org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)<br>java.lang.Thread.run(Thread.java:745)<br></pre><br>&#x8FD9;&#x4E2A;&#x8FC7;&#x7A0B;&#x4F7F;&#x5F97;hdfs&#x4ECE;&#x9519;&#x8BEF;&#x4E2D;&#x6062;&#x590D;&#x53D8;&#x5F97;&#x7F13;&#x6162;(&#x9700;&#x8981;2&#x4E2A;timeout&#x7684;&#x65F6;&#x95F4;&#xFF09;&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x4FEE;&#x6539;receivePacket&#x7684;&#x903B;&#x8F91;&#x6765;&#x5FEB;&#x901F;&#x7684;&#x53D1;&#x73B0;&#x8FD9;&#x6837;&#x7684;&#x60C5;&#x51B5;&#xFF1A;</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Sanity check the header</span></div><div class="line"><span class="comment">// change condition to if (header.getOffsetInBlock() &gt; replicaInfo.getBytesOnDisk()) {</span></div><div class="line"><span class="keyword">if</span> (header.getOffsetInBlock() &gt; replicaInfo.getNumBytes()) {</div><div class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Received an out-of-sequence packet for &quot;</span> + block + </div><div class="line">      <span class="string">&quot;from &quot;</span> + inAddr + <span class="string">&quot; at offset &quot;</span> + header.getOffsetInBlock() +</div><div class="line">      <span class="string">&quot;. Expecting packet starting at &quot;</span> + replicaInfo.getNumBytes());</div><div class="line">}</div></pre></td></tr></table></figure>
<p>&#x8FD9;&#x9700;&#x8981;&#x91CD;&#x542F;&#x6211;&#x4EEC;&#x7684;datanode&#x670D;&#x52A1;&#x624D;&#x80FD;&#x751F;&#x6548;&#x3002;</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;&amp;#x73B0;&amp;#x8C61;&quot;&gt;&lt;a href=&quot;#&amp;#x73B0;&amp;#x8C61;&quot; class=&quot;headerlink&quot; title=&quot;&amp;#x73B0;&amp;#x8C61;&quot;&gt;&lt;/a&gt;&amp;#x73B0;&amp;#x8C61;&lt;/h3&gt;&lt;p&gt;&amp;#x5728;&amp;#x621
    
    </summary>
    
    
  </entry>
  
</feed>
